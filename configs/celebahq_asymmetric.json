{
  "_comment": "Asymmetric autoencoder with no linear latent layer - keeps spatial structure throughout, using 511 filters at latent",
  "description": "Asymmetric CelebA-HQ Taxonomic Autoencoder: 5 encoder layers (256x256->16x16) and 4 decoder layers (16x16->256x256) with no linear bottleneck",
  "experiment_name": "celebahq_asymmetric_511",
  "data": {
    "data_root": "./data/celeba_hq",
    "batch_size": 64,
    "num_workers": 4,
    "val_split": 0.1,
    "train_subset": null,
    "val_subset": null,
    "_comment_subset": "Set train_subset/val_subset to an integer (e.g., 5000) to train on a smaller subset for faster iteration. null = use full dataset"
  },
  "model": {
    "latent_dim": 256,
    "temperature": 1.0,
    "use_maxpool": true,
    "output_activation": "sigmoid",
    "encoder_layers": [
      {
        "layer_type": "taxonomic_conv",
        "n_layers": 5,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "_comment": "256x256x3 -> 128x128x(2^5-1) = 128x128x31"
      },
      {
        "layer_type": "taxonomic_conv",
        "n_layers": 6,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "_comment": "128x128x31 -> 64x64x(2^6-1) = 64x64x63"
      },
      {
        "layer_type": "taxonomic_conv",
        "n_layers": 7,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "_comment": "64x64x63 -> 32x32x(2^7-1) = 32x32x127"
      },
      {
        "layer_type": "taxonomic_conv",
        "n_layers": 8,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "_comment": "32x32x127 -> 16x16x(2^8-1) = 16x16x255"
      },
      {
        "layer_type": "taxonomic_conv",
        "n_layers": 9,
        "kernel_size": 3,
        "stride": 1,
        "padding": 1,
        "_comment": "16x16x255 -> 16x16x(2^9-1) = 16x16x511 (latent space)"
      }
    ],
    "decoder_layers": [
      {
        "layer_type": "taxonomic_deconv",
        "n_layers": 8,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "output_padding": 1,
        "_comment": "16x16x511 -> 32x32x(2^8-1) = 32x32x255"
      },
      {
        "layer_type": "taxonomic_deconv",
        "n_layers": 7,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "output_padding": 1,
        "_comment": "32x32x255 -> 64x64x(2^7-1) = 64x64x127"
      },
      {
        "layer_type": "taxonomic_deconv",
        "n_layers": 6,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "output_padding": 1,
        "_comment": "64x64x127 -> 128x128x(2^6-1) = 128x128x63"
      },
      {
        "layer_type": "taxonomic_deconv",
        "n_layers": 5,
        "kernel_size": 3,
        "stride": 2,
        "padding": 1,
        "output_padding": 1,
        "_comment": "128x128x63 -> 256x256x(2^5-1) = 256x256x31, then final conv to 256x256x3"
      }
    ]
  },
  "training": {
    "epochs": 10,
    "learning_rate": 0.001
  },
  "analysis": {
    "checkpoint_path": "outputs/celebahq/training/celebahq_asymmetric_511/final_model.pt",
    "num_latent_batches": 50,
    "num_reconstruction_batches": 20,
    "num_multiple_recon_images": 8,
    "num_reconstructions_per_image": 8,
    "num_activation_images": 3
  },
  "output": {
    "training_save_dir": "outputs/celebahq/training/celebahq_asymmetric_511",
    "analysis_save_dir": "outputs/celebahq/analysis/celebahq_asymmetric_511"
  }
}
